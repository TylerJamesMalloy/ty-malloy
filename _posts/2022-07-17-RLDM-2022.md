Last month, from June 8-11, I attended and presented a poster at the Reinforcement Learning and Decision making conference. This is one of my favorite conferences and the only one that I have attended twice in person. Reinforcement Learning research can often be seen as significantly split between the human learning side and the computational application side. This conference is great for people interested in both of these areas, especially if they are interested in the intersection between them. 

I attended RLDM 2019 and presented my first accepted abstract and poster [Predicting Human Choice in a Multi-Dimensional N-Armed Bandit Task Using Actor-Critic Feature Reinforcement Learning](https://tylerjamesmalloy.github.io/ty-malloy/conference/2021/12/15/RLDM-2019-Lookback.html). Soon after this i began my joint research project with IBM on Human-Inspired Reinforcement Learning. I believe that the RLDM conference is emblematic of the type of research I was most interested in when I began this collaboration, and it served as a wealth of interesting topics across cognitive science and artificial intelligence. 

Many of the researchers I am most interested in attend and present at RLDM, or are even the organizers, and it's a great chance to see what everyone in the field has been working on. Typically these conferences run once every two years, but due to covid there hasn't been one since I first went in 2019, meaning there was a lot of work to update on. Also in that time Reinforcement Learning research in AI has seen an explosion in interest, as models have been applied to achievign super-human performance in a wide range of tasks. While those topics are very interesting, I am glad that the RLDM conference has maintained its connections to human learning as a signficiant focus of the talks and posters. 

This year, I presented the following poster, which applied the cognitive model of human learning and decision making that I have been working on as a part of my PhD thesis onto an existing data set collected from human participants learning in a visual task. These results are closely related to the presentation I gave to [VSS earlier this year](https://tylerjamesmalloy.github.io/ty-malloy/2022/05/28/VSS-2022.html), with added information and analysis of the learned representations of our proposed cogntitive model. 

<figure>
<img  src="https://raw.githubusercontent.com/TylerJamesMalloy/DisentangledVisualLearning/master/RLDM.png"  alt="RLDM"  style="width:100%"><figcaption  align = "center"><b>Poster presented to RLDM 2022</b></figcaption>
</figure>


To read the full abstract submission for RLDM 2022, you can [find it at this link to my researchgate profile](https://www.researchgate.net/publication/361654186_Modeling_Human_Reinforcement_Learning_with_Disentangled_Visual_Representations). 