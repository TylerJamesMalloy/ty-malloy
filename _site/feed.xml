<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-01-19T15:37:00-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Human and Machine Learning in Cognitive Science</title><subtitle>Tyler Malloy is a cognitive science PhD student and researcher studying human and machine learning. This site is for blog posts, research ideas, presentations and more.
</subtitle><author><name>Tyler Malloy</name><email>tylerjmalloy@gmail.com</email></author><entry><title type="html">Is Reward Enough?</title><link href="http://localhost:4000/review/2022/01/17/Reward-Review.html" rel="alternate" type="text/html" title="Is Reward Enough?" /><published>2022-01-17T00:00:00-05:00</published><updated>2022-01-17T00:00:00-05:00</updated><id>http://localhost:4000/review/2022/01/17/Reward-Review</id><content type="html" xml:base="http://localhost:4000/review/2022/01/17/Reward-Review.html"><![CDATA[<p>In this post I provide a review and opinion on the paper <a href="https://www.sciencedirect.com/science/article/pii/S0004370221000862">“Reward Is Enough”</a> by D Silver, S Singh, D Precup, and RS Sutton. In this work, the authors provide a broad perspective on reinforcement learning research and put forward the opinion that much of the behavior that interests cognitive science and artificial intelligence researchers can be viewed in relation to reward. Specifically, they propose that many cognitive faculties such as perception, language, generalization, imitation, and even general intelligence can be achieved through reward maximization and experience in an environment. They describe a hypothesis alongside these claims that is essentially stated in the short title, that reward is enough to learn these types of complex behaviors. The following figure borrowed from the paper describes several phenomenon which could hypothetically be trained through reward based reinforcement style learning.</p>

<figure>

<img src="https://ars.els-cdn.com/content/image/1-s2.0-S0004370221000862-gr001.jpg" alt="Trulli" style="width:100%" /><figcaption align="center"><b>Fig.1 - Reward Is Enough
David Silver, Satinder Singh, Doina Precup, Richard S.Sutton (paper on ScienceDirect). This figure demonstrates the overlap in behaviour that can conceviably be taught through reward signals in a cognitive vs. artificial agent.</b></figcaption>

</figure>

<p>For a cognitive psychologist or cognitive philosopher the first impression of this claim and the themes of the paper may be somewhat negative. Haven’t Chomsky and others taught us that experience in language use alone cannot give us the tools we need to be good language users? There should be a requirement that a universal grammar exist to reduce the hypothesis space of possible grammars before we even begin understanding the utterances of others, let alone generating our own. In point of fact there is a significant dearth of ‘negative’ examples of proper language, as most of our experience is with well formed language. Furthermore, much of our experience with language happens internally, without a clearly defined external reward signal.</p>

<p>Although these potential issues can be raised when taking the reward-is-enough hypothesis as a general claim or descriptive thesis on human cognition, in reality the goals of the paper are in showing that reward is sufficient for complex behavior learning. Because of this, the main purpose of the paper could be supported even without any evidence of reward based learning in a human agent. Instead the paper seeks to provide evidence that artificial reinforcement learning agents could hypothetically learn the complex behavior that humans achieve through reward signals alone.</p>

<p>While it is true that the paper claims to be more interested in describing the ‘sufficient’ aspect of learning behavior through reward, it does at the same time make some claims and connections to cognitive science that are more controversial. One source of potential controversy is the section entitled “What else, other than reward maximisation, could be enough for intelligence?” In this section the authors provide brief outlines of alternative hypotheses and suggest that they are not as well fit for training goals of general AI and other interesting behaviors. The presence of this section raises the question of the true intentions of the paper as a whole. If as the authors claim the hypothesis is centered around how behavior could be taught and not a description of human cognition, then why iterate through a list of alternatives and claim they cannot do what reward alone can?</p>

<p>In particular, the brief sentences on the free-energy hypothesis arguably leave out some important claims by cognitive psychologists such as Karl Friston in his paper “The free-energy principle: a unified brain theory?” The full quote from ‘Reward Is Enough’ is as follows: “Maximisation of free energy or minimisation of surprise may yield several abilities of natural intelligence, but does not provide a general-purpose intelligence that can be directed towards a broad diversity of different goals in different environments. Consequently it may also miss abilities that are demanded by the optimal achievement of any one of those goals (for example, aspects of social intelligence required to mate with a partner, or tactical intelligence required to checkmate an opponent).”</p>

<p>It seems from Friston’s perspective that free energy alone could provide much of what reward does for the authors of this paper. At least this should have more of a discussion if the authors are interested in showing why reward is unique in its ability. If they are not then it may be better to eschew a discussion of alternatives or claims that reward is specifically unique in its position. Otherwise, it is hard to not make larger connections of the claims made in the paper to a description of human cognition.</p>

<p>To me the general theme of the paper actually reminded me much of Karl Friston’s paper previously mentioned. There are many complex behaviors that could be defined as achieving something like free-energy minimization or reward maximization. Generally this is a good place to start for machine learning or artificial intelligence research because if you can begin to define what desirable behaviour looks like in terms that resemble something like a trainable loss function, then you are well on your way to making a useful system. This may be interesting from an AI researcher or engineering perspective, but for a cognitive psychologist the claims could be seen as somewhat vacuous and less useful for understanding cognition.</p>

<p>If you are interested in my perspective, I think that at least for much of human learning and decision making there is a combination of reward driven reinforcement style learning and predictive processing used for planning and beliefs about future states. However, I will add that I make no claims of how language and general intelligence specifically can and should be taught, as those are outside of the realm of my particular experience. I would imagine that much of those more complex behaviors is more driven by the structure of cognitive architectures as they have been optimized through millions of years of evolution.</p>]]></content><author><name>Tyler Malloy</name><email>tylerjmalloy@gmail.com</email></author><category term="review" /><summary type="html"><![CDATA[In this post I provide a review and opinion on the paper “Reward Is Enough” by D Silver, S Singh, D Precup, and RS Sutton. In this work, the authors provide a broad perspective on reinforcement learning research and put forward the opinion that much of the behavior that interests cognitive science and artificial intelligence researchers can be viewed in relation to reward. Specifically, they propose that many cognitive faculties such as perception, language, generalization, imitation, and even general intelligence can be achieved through reward maximization and experience in an environment. They describe a hypothesis alongside these claims that is essentially stated in the short title, that reward is enough to learn these types of complex behaviors. The following figure borrowed from the paper describes several phenomenon which could hypothetically be trained through reward based reinforcement style learning.]]></summary></entry><entry><title type="html">New Year: Looking Ahead to 2022</title><link href="http://localhost:4000/personal/2022/01/03/NewYear-2022-Lookahead.html" rel="alternate" type="text/html" title="New Year: Looking Ahead to 2022" /><published>2022-01-03T00:00:00-05:00</published><updated>2022-01-03T00:00:00-05:00</updated><id>http://localhost:4000/personal/2022/01/03/NewYear-2022-Lookahead</id><content type="html" xml:base="http://localhost:4000/personal/2022/01/03/NewYear-2022-Lookahead.html"><![CDATA[<p>Rather than look back at previous research I have done, as the previous posts on this blog have done, this post will look forward to my hopes for 2022 and new research ideas I am interested in. Firstly, the major plans for this year include completing the website hosting my thesis project, submitting a paper based on my work in Theory of Mind for reinforcement learning, and completing my PhD Thesis. Aside from that, on a more personal level I will be taking some time this year to look for possible post-doc positions or research centric positions in other areas. As a part of that I hope to continue with this blog and additionally go back to some of my previous posts and add a bit of information. Additionally I hope to expand on some of my background knowledge of other areas of cognitive science, psychology, and machine learning.</p>

<p>That all sounds like a lot of things! So this year is looking to be a busy one for me. However, I think that a lot of these goals are both interesting and necessary for my future plans, so I will try to take that motivation forward in pursuing these goals. Generally I am not one for clearly defined ‘new years resolutions’, because I try to improve myself at a slow and stedy pace, to increase the chance of making lasting changes and achieving long term goals. But I do think that it is good to take the time at the beginning of the year or even just the changing of the seasons to reflect on aspirations, goals, and so on. And there is no time like the last year of a PhD to reflect on these things!</p>

<p>Concretely, the priorities I have are focused around the completion of my PhD since I hope to do that with as few complications as possible, and my position currently affords a good opertunity for that. So my main focuses are towards the research most central to my PhD, as well as things that will be useful for my future as a researcher including relevant research for potential postdoc or other positions. Those specific goals definitely seem relatively focused, and I can always add on top of that some of the other ideas I have depending on whether I have the time for them.</p>

<p>Some of the personal aspirations I have other than research and job prospect related things are to read more fiction, as I do end up spending a good amount of time reading books and papers related to my research interests, and I’d like to get back into my hobby of science-fiction and fantasy. I did get a great christmas gift of two books by JRR Tolkien that I am excited to read, and I am also in the middle of a fantastic high-fantasy book called “The Priory of the Orange Tree” by Samantha Shannon. I hope to read those and other books this year, as I have been enjoying slowly getting back into reading for fun. I think it is useful to put down my thoughts and aspirations in this blog as it allows me to reflect on them more than simply thinking about them to myself.</p>]]></content><author><name>Tyler Malloy</name><email>tylerjmalloy@gmail.com</email></author><category term="personal" /><summary type="html"><![CDATA[Rather than look back at previous research I have done, as the previous posts on this blog have done, this post will look forward to my hopes for 2022 and new research ideas I am interested in. Firstly, the major plans for this year include completing the website hosting my thesis project, submitting a paper based on my work in Theory of Mind for reinforcement learning, and completing my PhD Thesis. Aside from that, on a more personal level I will be taking some time this year to look for possible post-doc positions or research centric positions in other areas. As a part of that I hope to continue with this blog and additionally go back to some of my previous posts and add a bit of information. Additionally I hope to expand on some of my background knowledge of other areas of cognitive science, psychology, and machine learning.]]></summary></entry><entry><title type="html">Modelling Learning and Decision Making Under Information Processing Constraints</title><link href="http://localhost:4000/thesis/2021/12/28/Thesis-2020-Lookback.html" rel="alternate" type="text/html" title="Modelling Learning and Decision Making Under Information Processing Constraints" /><published>2021-12-28T00:00:00-05:00</published><updated>2021-12-28T00:00:00-05:00</updated><id>http://localhost:4000/thesis/2021/12/28/Thesis-2020-Lookback</id><content type="html" xml:base="http://localhost:4000/thesis/2021/12/28/Thesis-2020-Lookback.html"><![CDATA[<p>This post is another retrospective, but instead of a conference or journal paper it takes a look at my masters thesis, titled “Modelling Learning and Decision Making Under Information Processing Constraints”. This blog post will go through the begninning stages of the project and how it ultimately narrowed down the focus of the thesis and project into what it eventually became.</p>

<p>Very early on in my PhD, I became interested my advisor Chris Sims’ previous work using infromation theory, specifically mutual information, as a tool to understand the cognitive costs of behaviour in learning and decision making tasks. While this may seem somewhat narrow, there are many possible applications of information theory in this way, and a wide range of psychological experimentation that has been done looking into modelling constrained cognition. We became interested in bandit learning tasks due to their simplicity, long history, and the publically available datasets with human participant responses. Since this was early on in my PhD and a while before I would eventually propose my own experiment to run, it made sense to test the ideas I had on another similar task that could guide my research in my PhD.</p>

<p>This inspired the project I worked on that would become an abstract paper in the Reinforcement Learning and Decision Making conference, and a slight extension of that paper is essentially the final section of my masters thesis. However, when I began the work in writing up my thesis, I realized I had become interested in the decision making setting during my economic modelling course. A large portion of this course was modelling decision making under risk and uncertainty, which is closely related to the decision making that takes place when learning in the bandit setting. I was wondering if the same concepts of mutual information and behavioural complexity could be used in this slightly different setting.</p>

<p>The key difference in the decision making setting is that the outcome utiltiies and probabilities that determine optimal behaviour are given directly, instead of being learned through experience as in the learning setting. This second interest and application ended up being roughly a third to a half of the content in my thesis, as it required a long background on decision making in various settings, which are some of the oldest and most well studied phenomenon in cognitive science. Taking a look back at my thesis, I realize it may seem like a lot of extra work to include only a slightly different phenomenon, but I am glad I did include it as it allowed me to relate and contrast my understanding of cognition with similar accounts from a wide variety of cognitive modells.</p>

<p>Broadly this experience in extending my ideas into related domains taught me that it is important to show as broad an application as possible, or at least as you find interesting. Often these models and approaches are applied to a very small domain which can make it difficult to relate to human cognition as we know it is extremely broad in its application. Additionally I was able to expand on my knowledge of cognitive modelling methods in similar domains as the ones I had experience in, which I am similarly greatful for.</p>]]></content><author><name>Tyler Malloy</name><email>tylerjmalloy@gmail.com</email></author><category term="Thesis" /><summary type="html"><![CDATA[This post is another retrospective, but instead of a conference or journal paper it takes a look at my masters thesis, titled “Modelling Learning and Decision Making Under Information Processing Constraints”. This blog post will go through the begninning stages of the project and how it ultimately narrowed down the focus of the thesis and project into what it eventually became.]]></summary></entry><entry><title type="html">Capacity-Limited Decentralized Actor-Critic for Multi-Agent Games</title><link href="http://localhost:4000/conference/2021/12/26/AAAI-2021-Lookback.html" rel="alternate" type="text/html" title="Capacity-Limited Decentralized Actor-Critic for Multi-Agent Games" /><published>2021-12-26T00:00:00-05:00</published><updated>2021-12-26T00:00:00-05:00</updated><id>http://localhost:4000/conference/2021/12/26/AAAI-2021-Lookback</id><content type="html" xml:base="http://localhost:4000/conference/2021/12/26/AAAI-2021-Lookback.html"><![CDATA[<p>This is the second post in a series of retrospectives on previous work I have done that shaped my PhD and are related to my future research goals. If you would like to read the paper you can find it on my <a href="https://www.researchgate.net/publication/354551984_Capacity-Limited_Decentralized_Actor-Critic_for_Multi-Agent_Games">Researchgate</a>.</p>

<p>Although this paper is relatively recent, I thought I would use it as my second post because it is closely realted to a project that I began soon after beginning my AI Researcher position with IBM in early 2019. I actually began this project before I presented the previous project at RLDM 2019. This earlier project, which eventually became an <a href="https://www.researchgate.net/publication/349345269_Consolidation_via_Policy_Information_Regularization_in_Deep_RL_for_Multi-Agent_Games">arxiv paper</a>, was influential in my interest in robotics, machine learning, and applications of reinforcement learning.</p>

<p>After this earlier project applying reinforcement learning to a robotics simulation environment, I extended the general idea into the project and paper that goes along with it “Capacity-Limited Decentralized Actor-Critic for Multi-Agent Games”. This was a fortunate paper as it us to have a published work on mutual information regularized reinforcement learning. This was the main idea around the earlier metod applied to robotics. Essentially the main idea is that we want to make the behaviour learned by robots as informationally simplistic as possible. This is inspired by human behaviour which we believe strives to be similarly informationally simplistic where possible.</p>

<p>This main idea was applied into a set of complex multi-agent environments with agents using a continuous control paradigm, as well as a degree of communication in some environments. We showed in our experimentation that applying a penalty to agent behavioural complexity at the learning stage can improve learning speed as well as the eventual highest performance of the agent. Compared to agents without this complexity constraint, our agents learned more generalizable behaviour that was simultaneously less informationally complex.</p>

<p>This was a great result for our ideas of human-inspired reinforcement leanring, and I will probably make a seperate blog post in the future explaining those ideas in the future. Since this project, our ideas of human-inspired RL have slightly shifted, but this was an important first step. Recently in our work applying lessons from how humans learn complex tasks onto reinforcement learning we have focused this ‘information constraint’ view in different ways, but in spirit this is the same concept as these earlier papers.</p>]]></content><author><name>Tyler Malloy</name><email>tylerjmalloy@gmail.com</email></author><category term="conference" /><summary type="html"><![CDATA[This is the second post in a series of retrospectives on previous work I have done that shaped my PhD and are related to my future research goals. If you would like to read the paper you can find it on my Researchgate.]]></summary></entry><entry><title type="html">Predicting Human Choice in a Multi-Dimensional N-Armed Bandit Task Using Actor-Critic Feature Reinforcement Learning</title><link href="http://localhost:4000/conference/2021/12/15/RLDM-2019-Lookback.html" rel="alternate" type="text/html" title="Predicting Human Choice in a Multi-Dimensional N-Armed Bandit Task Using Actor-Critic Feature Reinforcement Learning" /><published>2021-12-15T00:00:00-05:00</published><updated>2021-12-15T00:00:00-05:00</updated><id>http://localhost:4000/conference/2021/12/15/RLDM-2019-Lookback</id><content type="html" xml:base="http://localhost:4000/conference/2021/12/15/RLDM-2019-Lookback.html"><![CDATA[<p>This is the first post in a series of retrospectives looking back at papers and conferences I have attended. Now that I am entering my final year of my PhD, I will be begining this as a chronicle of projects, papers and conferences that influenced my time during my PhD.</p>

<p>This was my first accepted paper, I submitted it to the conference Reinforcement Learning and Decision Making in 2019. I attended the conference which was a great experience. I didn’t know then but it would be one of the few in-person conferences I would attend! Although many of the online conferences I had been to since have been great experiences, the value of in-person poster and talk presentations can’t be understated. I hope to go back to in-person conferences before the end of my PhD.</p>

<p>Now onto the contents of this paper. Looking back at it, I didn’t know it then but this would be a big impact on the direction of my research moving forward. This paper took a data set that had generously been made publically available by the <a href="https://nivlab.princeton.edu/">Niv Lab</a> at Princeton. Since it was my first year of my PhD, I didn’t have the direction yet to design an experiment with human participants, and this helped significantly. The motivation of my paper was looking into modelling human learning using more modern approaches that had recently been gaining attention in reinforcement learning research.</p>

<p>The specific method I looked into was actor-critic reinforcement learning, which had been used in both artificial intelligence research as well as some cognitive modelling approaches. The difference in predictive accruacy was modest, but from my perspective the paper serves to support the body of work that uses more complex and modern reinforcement learning and artificial intelligence methods for cognitive modelling. This contribution was the focus of my paper writing and my discussions during my poster presentation.</p>

<p>Since this conference, I have continued to use this dataset in testing the model I am currently working with on my PhD thesis. Additionally, the design of the learning task inspired in part the experiment that I will soon be running. It’s nice to look back at this early conference paper and think about how it has shaped my research moving forward.</p>

<p>Check out the <a href="https://www.researchgate.net/publication/335663034_Predicting_Human_Choice_in_a_Multi-Dimensional_N-Armed_Bandit_Task_Using_Actor-Critic_Feature_Reinforcement_Learning">paper on my researchgate profile here</a> if you are interested.</p>]]></content><author><name>Tyler Malloy</name><email>tylerjmalloy@gmail.com</email></author><category term="conference" /><summary type="html"><![CDATA[This is the first post in a series of retrospectives looking back at papers and conferences I have attended. Now that I am entering my final year of my PhD, I will be begining this as a chronicle of projects, papers and conferences that influenced my time during my PhD.]]></summary></entry></feed>